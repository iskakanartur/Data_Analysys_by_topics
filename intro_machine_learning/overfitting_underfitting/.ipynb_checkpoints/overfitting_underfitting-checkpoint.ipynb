{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generalization, Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### OVERFITTING ###########\n",
    "\n",
    "## And overfitting model uses its ability to capture complex patterns by being great at \n",
    "## predicting lots and lots of specific data samples or areas of local variation in the training set.\n",
    "\n",
    "## But it often misses seeing global patterns in the training set that would help it generalize well \n",
    "## on the unseen test set. It can't see these more global patterns because, intuitively, there's not \n",
    "## enough data to constrain the model to respect these global trends. \n",
    "\n",
    "## As a result, the training set accuracy is a hopelessly optimistic indicator for the likely test set \n",
    "## accuracy if the model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876a882",
   "metadata": {},
   "source": [
    "<img src=\"pics/o1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6dfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96972af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651003b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDERFITTING EXAMPLE\n",
    "\n",
    "## In this case the model has under-fit the data. The model is too simple for the actual trends that \n",
    "## are present in the data. It doesn't even do well on the training points. \n",
    "\n",
    "## So these blue points would represent the training points, the input to the training process for the \n",
    "## regression. And when we underfit, we have a model that's too SIMPLE, doesn't even do well on the training data\n",
    "## and thus, is not at all likely to generalize well to test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b7b59",
   "metadata": {},
   "source": [
    "<img src=\"pics/o3.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A better fit with a quadratic Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31249b1",
   "metadata": {},
   "source": [
    "<img src=\"pics/o4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54359ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## OVERFITTING EXAMPLE ###############\n",
    "\n",
    "## A third example might be, we might hypothesize that the relationship between the input variable and the\n",
    "## target variable is a function of several different parameters. Let's say, a polynomial so something that's \n",
    "## very bumpy. If we try to fit a more complex model to this set of training data, we might end up with \n",
    "## something that looks like this. \n",
    "\n",
    "## So, this more complex model has more parameters so it's able to capture more subtle behavior. \n",
    "## But it's also much higher variance here as you can see so it's more focused on capturing the more local \n",
    "## variations in the training data rather than trying to find the more global trend that we can see \n",
    "## as humans in the data. So, this is an example of overfitting. \n",
    "\n",
    "## In this case there's not enough data to really constrain the parameters of the model enough so that it \n",
    "## can recognize the global trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882536e",
   "metadata": {},
   "source": [
    "<img src=\"pics/o5.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ OVERFITTING IN CLASSIFICATION #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the decision boundry is the main task of classification \n",
    "## Normal Fit case will be below with a good boundry "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a547d4e",
   "metadata": {},
   "source": [
    "<img src=\"pics/o6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNDERFITTING IN CLASSIFICATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf463e2",
   "metadata": {},
   "source": [
    "<img src=\"pics/o7.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382052ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NORMAL FIT\n",
    "\n",
    "## A reasonably good model that fits well would be. You know, a linear model that finds this \n",
    "## general difference between the positive class over here and the negative class over here. \n",
    "\n",
    "## So you can see that it's AWARE  of this sort of GLOBAL  pattern of having most of the blue negative points in \n",
    "## the upper left and most of the red positive points more toward the lower right.\n",
    "\n",
    "## And it's robust enough in the sense that it ignores the fact that there may be occasionally a blue point in \n",
    "## the red region or red point in the blue region. \n",
    "## Instead, it's found this sort of more global separation between these two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de60bd9",
   "metadata": {},
   "source": [
    "<img src=\"pics/o8.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91746a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OVERFITTING IN CLASSIFICATION \n",
    "\n",
    "## An overfitting model on the other hand would typically be a model that has lots of parameters so it can \n",
    "## capture complex behavior. \n",
    "## And so, it would try to find something very clever, where it tried to completely separate the red points \n",
    "## from the blue points in a way that resulted in a highly variable decision boundary. \n",
    "\n",
    "## So this has the advantage that, a questionable advantage, that it does capture the training data classes \n",
    "## very well. \n",
    "## It predicts the training data classes almost perfectly. \n",
    "## But as you can see, if the actual division between the classes, it's captured by this linear model, t\n",
    "## he overfit model is going to make lots of mistakes in the regions where it's trying to be TOO PERFECT \n",
    "## in some sense. \n",
    "\n",
    "## So, you'll see this typically with overfitting. The overfit model is highly variable. \n",
    "## And again, it's trying to capture too many of the local fluctuations and does not have enough data to see \n",
    "## the global trend that would result in better overall generalization performance on new unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c456b03",
   "metadata": {},
   "source": [
    "<img src=\"pics/o9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d321a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ A VERY BRIEF INTRO TO K NEAREST NEIGHBORS ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This third example shows the effect of modifying the K parameter in the K nearest neighbors classifier. \n",
    "\n",
    "## The three plots shown here show the decision boundaries for K=10, K=5, and K=1. \n",
    "## And here we're using the fruit dataset again with the height of a piece of fruit on the x axis and the \n",
    "## width on the y axis. \n",
    "\n",
    "## So the K=10 case, as you recall, K=10 means that, for each query point that we want to get a prediction for, \n",
    "## let's say over here, we have to look at the 10 nearest points to the query point and we'll take those. \n",
    "## We won't go through all 10 here, but let's just say there are several here that are nearby and will average or\n",
    "## combine their votes to predict the label for this point. \n",
    "## So in the K=10 case, we need the votes from 10 different data instances in the training set to make our \n",
    "## prediction. \n",
    "\n",
    "## And as we reduce K, so K=5, we only need five neighbors to make a prediction. So for example, if the query \n",
    "## point was here, we'd look at these four and then possibly whatever was the closest in this let's say, \n",
    "## that one or this one. \n",
    "\n",
    "## And then finally, in the K=1 case, that's the most unstable in the sense that for any query point, \n",
    "## we only look at the single nearest neighbor to that point. \n",
    "\n",
    "## And so, the effect of reducing K in the k-nearest neighbors classifier is to increase the variance \n",
    "## of the decision boundaries, because the decision boundary can be affected by outliers. \n",
    "\n",
    "## If there's a point far, far away, it might have, it has much greater effect on the decision boundary \n",
    "## in the K=1 case, than it would in the K=10 case, when the votes of nine other neighbors are also needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec75327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e606fe6d",
   "metadata": {},
   "source": [
    "<img src=\"pics/o10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930f80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"pics/o6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177f109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"pics/o6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e6f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0fe85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f1d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
